@inproceedings{bittnermelody,
  title={Melody extraction by contour classification},
  author={Bittner, Rachel M and Salamon, Justin and Essid, Slim and Bello, Juan P},
  booktitle={Proc. ISMIR},
  pages={500--506}
}

@inproceedings{krebs2015efficient,
  title={An Efficient State-Space Model for Joint Tempo and Meter Tracking.},
  author={Krebs, Florian and B{\"o}ck, Sebastian and Widmer, Gerhard},
  booktitle={ISMIR},
  pages={72--78},
  year={2015}
}

@article{ryynanen2004probabilistic,
  title={Probabilistic modelling of note events in the transcription of monophonic melodies},
  author={Ryyn{\"a}nen, Matti},
  year={2004},
  publisher={Citeseer}
}

@inproceedings{rocamora2007comparing,
  title={Comparing audio descriptors for singing voice detection in music audio files},
  author={Rocamora, Mart{\i}n and Herrera, Perfecto},
  booktitle={Brazilian Symposium on Computer Music, 11th. San Pablo, Brazil},
  volume={26},
  pages={27},
  year={2007}
}

@inproceedings{lehner2014reduction,
  title={On the reduction of false positives in singing voice detection},
  author={Lehner, Bernhard and Widmer, Gerhard and Sonnleitner, Reinhard},
  booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7480--7484},
  year={2014},
  organization={IEEE}
}

@conference {Bosch,
	title = {A Comparison of Melody Extraction Methods Based on Source-Filter Modelling},
	booktitle = {17th International Society for Music Information Retrieval Conference (ISMIR 2016)},
	year = {2016},
	month = {Aug.},
	address = {New York},
	author = {Bosch, J. and Bittner, R. M. and Salamon, J. and G{\'o}mez, E.}
}

@article{salamon2014melody,
  title = {Melody Extraction from Polyphonic Music Signals: Approaches, Applications and Challenges},
  journal = {IEEE Signal Processing Magazine},
  volume = {31},
  year = {2014},
  month = {02/2014},
  pages = {118-134},
  abstract = {Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording. Over the past decade melody extraction has emerged as an active research topic, comprising a large variety of proposed algorithms spanning a wide range of techniques. This article provides an overview of these techniques, the applications for which melody extraction is useful, and the challenges that remain. We start with a discussion of {\textquoteleft}melody{\textquoteright} from both musical and signal processing perspectives, and provide a case study which interprets the output of a melody extraction algorithm for specific excerpts. We then provide a comprehensive comparative analysis of melody extraction algorithms based on the results of an international evaluation campaign. We discuss issues of algorithm design, evaluation and applications which build upon melody extraction. Finally, we discuss some of the remaining challenges in melody extraction research in terms of algorithmic performance, development, and evaluation methodology.},
  doi = {0.1109/MSP.2013.2271648},
  url = {files/publications/Salamon_Gomez_Ellis_Richard_MelodyExtractionReview_IEEESPM_2013.pdf},
  author = {Justin Salamon and G{\'o}mez, Emila and Ellis, Dan and Richard, Ga{\"e}l}
}

@inproceedings{rao2011context,
  title={Context-aware features for singing voice detection in polyphonic music},
  author={Rao, Vishweshwara and Gupta, Chitralekha and Rao, Preeti},
  booktitle={International Workshop on Adaptive Multimedia Retrieval},
  pages={43--57},
  year={2011},
  organization={Springer}
}

@article{johnson2005capacity,
  title={Capacity and complexity of {HMM} duration modeling techniques},
  author={Johnson, Michael T},
  journal={Signal Processing Letters, IEEE},
  volume={12},
  number={5},
  pages={407--410},
  year={2005},
  publisher={IEEE}
}

@article{kroher2015automatic,
  title={Automatic Transcription of Flamenco Singing from Polyphonic Music Recordings},
  author={Kroher, Nadine and G{\'o}mez, Emilia},
  journal={IEEE/ACM Transactions on Audio, Speech and Language Processing},
  volume={24},
  number={5},
  pages={901--913},
  year={2016}
}

@article{fujihara2012lyrics,
  title={Lyrics-to-audio alignment and its application},
  author={Fujihara, Hiromasa and Goto, Masataka},
  journal={Dagstuhl Follow-Ups},
  volume={3},
  year={2012},
  publisher={Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik}
}

@inproceedings{gong2015real,
  title = {Real-Time Audio-to-Score Alignment of Singing Voice Based on Melody and Lyric Information},
  booktitle = {Interspeech 2015},
  year = {2015},
  month = {06/09/2015},
  address = {Dresden, Germany},
  abstract = {Singing voice is specific in music: a vocal performance con- veys both music (melody/pitch) and lyrics (text/phoneme) con- tent. This paper aims at exploiting the advantages of melody and lyric information for real-time audio-to-score alignment of singing voice. First, lyrics are added as a separate observa- tion stream into a template-based hidden semi-Markov model (HSMM), whose observation model is based on the construc- tion of vowel templates. Second, early and late fusion of melody and lyric information are processed during real-time audio-to-score alignment. An experiment conducted with two professional singers (male/female) shows that the performance of a lyrics-based system is comparable to that of melody-based score following systems. Furthermore, late fusion of melody and lyric information substantially improves the alignment per- formance. Finally, maximum a posteriori adaptation (MAP) of the vowel templates from one singer to the other suggests that lyric information can be efficiently used for any singer.},
  author = {Rong Gong and Philippe Cuvillier and Nicolas Obin and Arshia Cont}
}

@inproceedings{atli2014audio,
  abstract = {For Turkish makam music, there exist several analysis tools which generally use only the audio as the input to extract the features of the audio. This study aims at extending such approach by using additional features such as scores, editorial metadata and the knowledge about the music. In this paper, the existing algorithms for similar research, the improvements we apply to the existing audio feature extraction tools and some potential topics for audio feature extraction of Turkish makam music are explained. For the improvements, we make use of the Turkish makam music corpus and the culture specific knowledge. We also present a web-based platform, Dunya, where the output of our research, such as pitch histograms, melodic progressions and segmentation information will be used to explore a collection of audio recordings of Turkish makam music.},
  address = {Ankara, Turkey},
  author = {Atl{\i}, Hasan Sercan and Uyar, Burak and Sertan {\c S}ent{\"u}rk and Bozkurt, Bar{\i}{\c s} and Serra, Xavier},
  title = {Audio Feature Extraction for Exploring {Turkish} Makam Music},
  booktitle = {3rd International Conference on Audio Technologies for Music and Media},
  publisher = {Bilkent University},
  organization = {Bilkent University},
  url = {http://sertansenturk.com/uploads/publications/atli2014feature_atmm.pdf},
  year = {2014}
}

@article{gomez2013towards,
  title={Towards computer-assisted flamenco transcription: An experimental comparison of automatic transcription algorithms as applied to a cappella singing},
  author={G{\'o}mez, Emilia and Bonada, Jordi},
  journal={Computer Music Journal},
  volume={37},
  number={2},
  pages={73--90},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{raffel2014mir_eval,
  title={mir\_eval: A transparent implementation of common MIR metrics},
  author={Raffel, Colin and McFee, Brian and Humphrey, Eric J and Salamon, Justin and Nieto, Oriol and Liang, Dawen and Ellis, Daniel PW and Raffel, C Colin},
  booktitle={In Proceedings of the 15th International Society for Music Information Retrieval Conference, ISMIR},
  year={2014},
  organization={Citeseer}
}



@article{krige2008explicit,
  title={Explicit transition modelling for automatic singing transcription},
  author={Krige, Willie and Herbst, Theo and Niesler, Thomas},
  journal={Journal of New Music Research},
  volume={37},
  number={4},
  pages={311--324},
  year={2008},
  publisher={Taylor \& Francis}
}

@article{ishwar2014pitch,
  title={Pitch Estimation of the Predominant Vocal Melody from Heterophonic Music Audio Recordings},
  author={Ishwar, Vignesh},
  year={2014}
}

@article{sundberg2006kth,
  title={The {KTH} synthesis of singing},
  author={Sundberg, Johan},
  journal={Advances in {Cognitive} Psychology},
  volume={2},
  number={2-3},
  pages={131--143},
  year={2006},
  publisher={Wy{\.z}sza Szko{\l}a Finans{\'o}w i Zarz{\k{a}}dzania w Warszawie}
}


@inproceedings{mauch2015computer,
  title={Computer-aided melody note transcription using the Tony software: Accuracy and efficiency},
  author={Mauch, Matthias and Cannam, Chris and Bittner, Rachel and Fazekas, George and Salamon, Justin and Dai, Jiajie and Bello, Juan and Dixon, Simon},
  booktitle={Proceedings of the First International Conference on Technologies for Music Notation and Representation (TENOR 2015)},
  pages={23--30},
  year={2015}
  }


@article{mauch2012integrating,
  title={Integrating additional chord information into HMM-based lyrics-to-audio alignment},
  author={Mauch, Matthias and Fujihara, Hiromasa and Goto, Masataka},
  journal={Audio, Speech, and Language Processing, IEEE Transactions on},
  volume={20},
  number={1},
  pages={200--210},
  year={2012},
  publisher={IEEE}
}

@inproceedings{holzapfel2014tracking,
  title={Tracking the" Odd": Meter Inference in a Culturally Diverse Music Corpus.},
  author={Holzapfel, Andre and Krebs, Florian and Srinivasamurthy, Ajay},
  booktitle={ISMIR},
  pages={425--430},
  year={2014}
}

@article{holzapfel2015relation,
  title={Relation between surface rhythm and rhythmic modes in Turkish makam music},
  author={Holzapfel, Andr{\'e}},
  journal={Journal of New Music Research},
  volume={44},
  number={1},
  pages={25--38},
  year={2015},
  publisher={Taylor \& Francis}
}


% This file was created with JabRef 2.9.2.
% Encoding: MacRoman

@ARTICLE{sertanSectionLinking2014,
  author = {Sertan {\c S}ent{\"u}rk and Andr{\'e} Holzapfel and Serra, Xavier},
  title = {Linking Scores and Audio Recordings in Makam Music of Turkey},
  journal = {Journal of New Music Research},
  year = {In Press},
  abstract = {The most relevant representations of music are notations and audio
  recordings, each of which emphasizes a particular perspective and
  promotes different approximations in the analysis and understanding
  of music. Linking these two representations and analyzing them jointly
  should help to better study many musical facets by being able to
  combine complementary analysis methodologies. In order to develop
  accurate linking methods, we have to take into account the specificities
  of a given type of music. In this paper, we present a method for
  linking musically relevant sections in a score of a piece from makam
  music of Turkey (MMT) to the corresponding time intervals of an audio
  recording of the same piece. The method starts by extracting relevant
  features from the score and from the audio recording. The features
  of a given score section are compared with the features of the audio
  recording to find the candidate links in the audio for that score
  section. Next, using the sequential section information stored in
  the score, it selects the most likely links. The method is tested
  on a dataset consisting of instrumental and vocal compositions of
  MMT, achieving 92.1\% and 96.9\% F1-scores on the instrumental and
  vocal pieces, respectively. Our results show the importance of culture-specific
  and knowledge-based approaches in music information processing.}
}

@INPROCEEDINGS{durrieu2009main,
  author = {Durrieu, Jean-Louis and Ozerov, Alexey and F{\'e}votte, C{\'e}dric
  and Richard, Ga{\"e}l and David, Bertrand},
  title = {Main instrument separation from stereophonic audio signals using
  a source/filter model},
  booktitle = {European Signal Processing Conference (EUSIPCO), Glasgow, Scotland},
  year = {2009}
}

@ARTICLE{FilipsThesis,
  author = {Filip},
  title = {MAsters Thesis},
  review = {The phonetic features are : 
  
  a vector of the 9 log-probabilities of the 9 vocals in the vocal trapezoid
  
  for a given vocal a model with these 9 probabilitiesis trained using
  the vocal joystick voiwe corpus.
  
  
  It has been proposed a HMM that has 2 dimensions - horizaontal note
  states dimension
  
  and 
  
  vertical - phoneme state duration
  
  HOWEVER! this model is not applied but a simple mmoel : 
  
  The vowel information is incorporated in the output distributions
  using only a single stationary manifestation per note
  
  
  ------------------
  
  the vowel and the pitch features are mutually independent. ... This
  justifies the fact that we can consider them as two independent sets
  of features
  
  
  According to the method found in Rap99 the output probability of a
  combined feature vector can be represented as the product of output
  probability of the two independent feature sets. 
  
  Since a feature set is driven by an indep. Gaussian, we can combine
  them straightforward into a common Gaussian.},
  timestamp = {2013.05.07}
}

@ARTICLE{karaosmanouglu2012{Turkish},
  author = {Karaosmano{\u{g}}lu, M Kemal},
  title = {A {Turkish} makam music symbolic database for music information retrieval:
  Symbtr},
  journal = {Proceedings of the 13th International Society for Music Information Retrieval Conference (ISMIR)},
  year = {2012}
}

@INPROCEEDINGS{mesaros2008automatic,
  author = {Mesaros, Annamaria and Virtanen, Tuomas},
  title = {Automatic alignment of music audio and lyrics},
  booktitle = {Proceedings of the 11th Int. Conference on Digital Audio Effects
  (DAFx-08)},
  year = {2008},
  review = {viterbi decoding used, but restricted to one string of phonemes.
  
  manual alignment of start of lyrial lines to text made. 
  
  
  there is first vocal line extraction model
  
  
  model: 
  
  3-state phonemes from speech corpus, adapted using MLLR to clean singing
  voice. 
  
  + 
  
  background noise model having different amoount of states. 
  
  
  
  -----
  
  Problems of the lyrics-audio alignment task: 
  
  1) there are significant differences in the dynamics and general properties
  of speech and singing sounds. 
  
  2) The com- plexity of the polyphonic music signal compared to a pure
  singing voice signal}
}

@ARTICLE{Salor2007580,
  author = {\"{O}zg\"{u}l Salor and Bryan L Pellom and Tolga \c{C}ilo\u{g}lu and M\"{u}beccel Demirekler},
  title = {{Turkish} speech corpora and recognition tools developed by porting
  SONIC: Towards multilingual speech recognition },
  journal = {Computer Speech and Language },
  year = {2007},
  volume = {21},
  pages = {580 - 593},
  number = {4},
  doi = {http://dx.doi.org/10.1016/j.csl.2007.01.001},
  issn = {0885-2308},
  keywords = {Phonetic aligner},
  url = {http://www.sciencedirect.com/science/article/pii/S0885230807000022}
}

@INPROCEEDINGS{senturk2012approach,
  author = {\c{S}ent{\"u}rk, Sertan and Holzapfel, Andr{\'e} and Serra, Xavier},
  title = {An approach for linking score and audio recordings in Makam music
  of Turkey},
  booktitle = {Serra X, Rao P, Murthy H, Bozkurt B, editors. Proceedings of the
  2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:
  Universitat Pompeu Fabra; 2012. p. 95-106.},
  year = {2012},
  organization = {Universitat Pompeu Fabra}
}

@INPROCEEDINGS{senturk2012approach,
  author = {\c{S}ent{\"u}rk, Sertan and Holzapfel, Andr{\'e} and Serra,
  Xavier},
  title = {An Approach for Linking Score and Audio Recordings in Makam Music
  of Turkey},
  booktitle = {Serra X, Rao P, Murthy H, Bozkurt B, editors. Proceedings of the
  2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:
  Universitat Pompeu Fabra; 2012. p. 95-106.},
  year = {2012},
  organization = {Universitat Pompeu Fabra},
  review = {different tunings and extensive usage of non-notated expressive elements.
  
   Addresses the peculiarities in the musical properties of makams in
  conrast to western music
  
  e.g. : non-notated expressive elements (e.g. embelishments), change
  in tuning
  
  
  Starting and ending of sections are provided from the musical score.
  
  
  A synthetic pitch contour is synthesized from the notes, to which
  marks for the section boundaries are linked.
  
  Then the synthetic pitch contour is matched to a f0-contour extracted
  from the audio signal. The result of the matching are time locations
  in the audio, which correspond to the section boundaries.
  
  
  Methodology in detail: 
  
  From the audio recording, the fundamental frequency, f0,
  
  is estimated and processed to obtain an audio pitch contour. The f0
  estimation is also used to calculate a pitch
  
  histogram in order to identify the tuning and the note intervals (Section
  3.2.1). From the score information, we read
  
  the note symbols, the sections and the makam of the piece,
  
  and generate a synthetic pitch contour (Section 3.2.2). In
  
  order to estimate the candidate locations of the sections in
  
  the audio, the method compares these relevant pitch representations
  (Section 3.3). In the Þnal step, the candidates
  
  are hierarchically checked to link the sections of the score
  
  to the corresponding parts in the audio (Section 3.4).}
}

@CONFERENCE{CompMusic2276,
  author = {Serra, Xavier},
  title = {A Multicultural Approach in Music Information Research},
  booktitle = {Proceedings of the 12th International Society for Music Information Retrieval Conference},
  year = {2011},
  pages = {151-156},
  address = {Miami, Florida (USA)},
  month = {24/10/11},
  abstract = {<p>Our information technologies do not respond to the world\&$\#$39;s
  multicultural reality; in fact, we are imposing the paradigms of
  our market-driven western culture also on IT, thus facilitating the
  access of a small part of the world\&rsquo;s information to a small
  part of the world\&$\#$39;s population. The current IT research efforts
  may even make it worse, and future IT will accentuate this information
  bias. Most IT research is being carried out with a western centered
  approach and as a result, most of our data models, cognition models,
  user models, interaction models, ontologies, etc., are culturally
  biased. This fact is quite evident in music information re-search,
  since, despite the world\&$\#$39;s richness in terms of musi-cal
  culture, most research is centered on CDs and metadata of western
  commercial music. This is the motivation behind a large and ambitious
  project funded by the European Research Council entitled \&quot;CompMusic:
  Computational Models for the discovery of the world\&$\#$39;s music.\&quot;
  In this paper we present the ideas supporting this project, the challenges
  that we want to work on, and the proposed approaches to tackle these
  challenges.</p>},
  keywords = {CompMusic, Multiculturalism},
  url = {system/files/publications/Serra-Xavier-CompMusic-ISMIR-2011.pdf}
}

@INPROCEEDINGS{sordo2012musically,
  author = {Sordo, Mohamed and Koduri, Gopala Krishna and Sent{\"u}rk, Sertan
  and Gulati, Sankalp and Serra, Xavier},
  title = {A Musically aware system for browsing and interacting with audio
  music collections},
  booktitle = {Serra X, Rao P, Murthy H, Bozkurt B, editors. Proceedings of the
  2nd CompMusic Workshop; 2012 Jul 12-13; Istanbul, Turkey. Barcelona:
  Universitat Pompeu Fabra; 2012.},
  year = {2012},
  organization = {Universitat Pompeu Fabra},
  review = {distance measures are needed to navigate through the different information
  objects. 
  
  It is desired that the system is able to explore all the musical elements
  of a collection of music. An example is searching by a musical phrase,
  rhythmic pattern or an expressive articulation of vocals. The latter
  is a musical aspect that is dependent on the lyrics to the song.
  To enable a comparison between a query of a certain articulation
  of vocals and a set of target articulations, a musically-aware similarity
  metric has to be defined.
  
  
  --- 
  
  features extracted so far.}
}

@INPROCEEDINGS{wang2004lyrically,
  author = {Wang, Ye and Kan, Min-Yen and Nwe, Tin Lay and Shenoy, Arun and Yin,
  Jun},
  title = {LyricAlly: automatic synchronization of acoustic musical signals
  and textual lyrics},
  booktitle = {Proceedings of the 12th annual ACM international conference on Multimedia},
  year = {2004},
  pages = {212--219},
  organization = {ACM},
  review = {three modules: 
  
  
  beat detector -> chorus Detector -> vocal detector 
  
  
  Section 4.3. Vocal detector. 
  
  Based on HMM of vocal only and non-vocal spectral distribution. Time
  resolution: inter-beat interval.}
}

@ARTICLE{,
  title = {masataka article},
  owner = {joro},
  review = {It is known that the singing voice has more
  
  complicated frequency and dynamic characteristics than speech [20].
  For example,
  
  ßuctuation of fundamental frequency (F0) 2 and loudness of singing
  voices are far stronger
  
  than those of speech sounds.},
  timestamp = {2013.05.01},
  url = {http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf}
}




% This file was created with JabRef 2.10.
% Encoding: MacRoman


@InProceedings{dittmar2012towards,
  Title                    = {Towards lyrics spotting in the SyncGlobal project},
  Author                   = {Dittmar, Christian and Mercado, Pedro and Grossmann, Holger and Cano, Estefan{\i}a},
  Booktitle                = {Cognitive Information Processing (CIP), 2012 3rd International Workshop on},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {1--6}
}

@Article{duda1972use,
  Title                    = {Use of the {H}ough transformation to detect lines and curves in pictures},
  Author                   = {Duda, Richard O and Hart, Peter E},
  Journal                  = {Communications of the ACM},
  Year                     = {1972},
  Number                   = {1},
  Pages                    = {11--15},
  Volume                   = {15},

  Publisher                = {ACM}
}

@Conference{Dzhambazov,
  Title                    = {Modeling of Phoneme Durations for Alignment between Polyphonic Audio and Lyrics},
  Author                   = {Georgi Dzhambazov and Serra, Xavier},
  Booktitle                = {Sound and Music Computing Conference},
  Year                     = {2015},

  Address                  = {Maynooth, Ireland},

  Abstract                 = {In this work we propose how to modify a standard scheme for text-to-speech alignment for the alignment of lyrics and singing voice. To this end we model the duration of phonemes specific for the case of singing. We rely on a duration-explicit hidden Markov model (DHMM) phonetic recognizer based on mel frequency cepstral coefficients (MFCCs), which are extracted in a way robust to background instrumental sounds. The proposed approach is tested on polyphonic audio from the classical {Turkish} music tradition in two settings: with and without modeling phoneme durations. Phoneme durations are inferred from sheet music. In order to assess the impact of the polyphonic setting, alignment is evaluated as well on an acapella dataset, compiled especially for this study. We show that the explicit modeling of phoneme durations improves alignment accuracy by absolute 10 percent on the level of lyrics lines (phrases) and performs on par with state-of-the-art aligners for other languages.}
}

@InProceedings{fujihara2008hyperlinking,
  Title                    = {Hyperlinking Lyrics: A Method for Creating Hyperlinks Between Phrases in Song Lyrics},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun},
  Booktitle                = {Proceedings of the 9th International Conference on Music Information Retrieval},
  Year                     = {2008},

  Address                  = {Philadelphia, USA},
  Month                    = {September 14-18},
  Pages                    = {281--286}
}

@InProceedings{hansen2012recognition,
  Title                    = {Recognition of Phonemes in A-cappella Recordings using Temporal Patterns and Mel Frequency Cepstral Coefficients},
  Author                   = {Hansen, Jens Kofod},
  Year                     = {2012},
  booktitle = {Proceedings of the 9th Sound and Music Computing Conference},
  address = {Copenhagen, Denmark},
  pages ={494--499}
}

@Conference{holzapfelsection,
  Title                    = {Section-level Modeling of Musical Audio for Linking Performances to Scores in {Turkish} Makam Music},
  Author                   = {Holzapfel, Andr{\'e} and {\c S}im{\c s}ekli, Umut and Sertan {\c S}ent{\"u}rk and Cemgil, Ali Taylan},
  Booktitle                = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2015},

  Address                  = {Brisbane, Australia},
  Month                    = {19/04/2015},

  Abstract                 = {Section linking aims at relating structural units in the notation of a piece of music to their occurrences in a performance of the piece. In this paper, we address this task by presenting a score-informed hierarchical Hidden Markov Model (HHMM) for modeling musical audio signals on the temporal level of sections present in a composition, where the main idea is to explicitly model the long range and hierarchical structure of music signals. So far, approaches based on HHMM or similar methods were mainly developed for a note-to-note alignment, i.e. an alignment based on shorter temporal units than sections. Such approaches, however, are conceptually problematic when the performances differ substantially from the reference score due to interpretation and improvisation, a very common phenomenon, for instance, in {Turkish} makam music. In addition to having low computational complexity compared to note-to-note alignment and achieving a transparent and elegant model, the experimental results show that our method outperforms a previously presented approach on a {Turkish} makam music corpus.},
  Url                      = {http://sertansenturk.com/uploads/publications/holzapfel2015linking_icassp.pdf}
}

@InProceedings{krebs2013rhythmic,
  Title                    = {Rhythmic pattern modeling for beat and downbeat tracking in musical audio},
  Author                   = {Krebs, Florian and B{\"o}ck, Sebastian and Widmer, Gerhard},
  Booktitle                = {Proceedings of the 14th International Society for Music Information Retrieval Conference},
  Year                     = {2013},

  Address                  = {Curitiba, Brazil},
  Month                    = {November 4-8}
}

@InProceedings{kruspekeyword,
  Title                    = {Keyword Spotting in A-capella Singing},
  Author                   = {Kruspe, Anna M},
  Booktitle                = {Proceedings of the 15th International Society for Music Information Retrieval Conference},
  Year                     = {2014},

  Address                  = {Taipei, Taiwan},
  Pages                    = {271-276},

  Review                   = {Approach: 
first step : phoneme Recognition by MLP. sperately with 3 different features : MFCC, PLP and TRAP
second step: postprocessing and combining of features
third HMM-based keyword spotting. as input used postetior probs from HMMs

\
Dataset: 19 clean singing vocie recordings

Conslucion: MLP trained on singing voice better than MLP trained on speech}
}

@Article{levy2008structural,
  Title                    = {Structural segmentation of musical audio by constrained clustering},
  Author                   = {Levy, Mark and Sandler, Mark},
  Journal                  = {Audio, Speech, and Language Processing, IEEE Transactions on},
  Year                     = {2008},
  Number                   = {2},
  Pages                    = {318--326},
  Volume                   = {16},

  Publisher                = {IEEE}
}

@Book{muller2007information,
  Title                    = {Information retrieval for music and motion},
  Author                   = {M{\"u}ller, Meinard},
  Publisher                = {Springer},
  Year                     = {2007},
  Volume                   = {2}
}

@Article{mandal2014recent,
  Title                    = {Recent developments in spoken term detection: a survey},
  Author                   = {Mandal, Anupam and Kumar, KR Prasanna and Mitra, Pabitra},
  Journal                  = {International Journal of Speech Technology},
  Year                     = {2014},
  Number                   = {2},
  Pages                    = {183--198},
  Volume                   = {17},

  Publisher                = {Springer}
}

@Book{Manning:2008:IIR:1394399,
  Title                    = {Introduction to Information Retrieval},
  Author                   = {Manning, Christopher D. and Raghavan, Prabhakar and Sch\"{u}tze, Hinrich},
  Publisher                = {Cambridge University Press},
  Year                     = {2008},

  Address                  = {New York, NY, USA},

  ISBN                     = {0521865719, 9780521865715}
}

@Article{mcnab1995signal,
  Title                    = {Signal processing for melody transcription},
  Author                   = {McNab, Rodger J and Smith, Lloyd A and Witten, Ian H},
  Year                     = {1995},

  Publisher                = {University of Waikato, Department of Computer Science},
  Review                   = {several pitch based and amplitude based segmentation methods}
}

@Article{molina2015sipth,
  Title                    = {Sipth: Singing transcription based on hysteresis defined on the pitch-time curve},
  Author                   = {Molina, Emilio and Tard{\'o}n, Lorenzo J and Barbancho, Ana M and Barbancho, Isabel},
  Journal                  = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  Year                     = {2015},
  Number                   = {2},
  Pages                    = {252--263},
  Volume                   = {23},

  Publisher                = {IEEE}
}

@PhdThesis{murphy2002dynamic,
  Title                    = {Dynamic bayesian networks: representation, inference and learning},
  Author                   = {Murphy, Kevin Patrick},
  School                   = {University of California},
  Year                     = {2002}
}

@InProceedings{sertanComposition,
  Title                    = {Composition Identification},
  Author                   = {Senturk}
}

@InProceedings{szoke2005comparison,
  Title                    = {Comparison of keyword spotting approaches for informal continuous speech.},
  Author                   = {Sz{\"o}ke, Igor and Schwarz, Petr and Matejka, Pavel and Burget, Luk{\'a}s and Karafi{\'a}t, Martin and Fapso, Michal and Cernock{\`y}, Jan},
  Booktitle                = {Interspeech},
  Year                     = {2005},
  Pages                    = {633--636}
}

@InProceedings{TurnbullEtAl_2007_ASupeApprFor,
  Title                    = {A Supervised Approach for Detecting Boundaries in Music Using Difference Features and Boosting},
  Author                   = {Turnbull, Douglas and Lanckriet, Gert and Pampalk, Elias and Goto, Masataka},
  Booktitle                = {Proceedings of the 8th International Conference on Music Information Retrieval},
  Year                     = {2007},

  Address                  = {Vienna, Austria},
  Month                    = {September 23-27},
  Pages                    = {51--54}
}

@InProceedings{von2010perceptual,
  Title                    = {Perceptual audio features for unsupervised key-phrase detection},
  Author                   = {Von Zeddelmann, Dirk and Kurth, Frank and M{\"u}ller, M},
  Booktitle                = {Acoustics Speech and Signal Processing (ICASSP), 2010 IEEE International Conference on},
  Year                     = {2010},
  Organization             = {IEEE},
  Pages                    = {257--260},

  Review                   = {Approach to find occurences of key phrases from speech. 
Speaker independent and unsupervised. 

Uses as query spoken phrase, not textual query. 
 
features: HFCC 

search strategy : the diagonal matching [4] - line segements assuming same duration. 
it can cope with 10% change of speed}
}

@InProceedings{WhiteleyEtAl_2006_BayeModeOfTemp,
  Title                    = {Bayesian Modelling of Temporal Structure in Musical Audio},
  Author                   = {Whiteley, Nick and Cemgil, A. Taylan and Godsill, Simon},
  Booktitle                = {Proceedings of the 7th International Conference on Music Information Retrieval},
  Year                     = {2006},

  Address                  = {Victoria (BC), Canada},
  Month                    = {October 8-12}
}

@Article{yu2010hidden,
  Title                    = {Hidden semi-{Markov} models},
  Author                   = {Yu, Shun-Zheng},
  Journal                  = {Artificial Intelligence},
  Year                     = {2010},
  Number                   = {2},
  Pages                    = {215--243},
  Volume                   = {174},

  Publisher                = {Elsevier}
}

@Article{dzahmbazovLyricalDur,
  Title                    = {ON THE USE OF LYRICAL DURATION FROM MUSICAL SCORE FOR AUTOMATIC LYRICS-TO-AUDIO ALIGNMENT},

  Owner                    = {dzhambazov}
}

@Unpublished{singing_separation,
  Title                    = {singing_separation},

  Owner                    = {joro},
  Timestamp                = {2014.02.06},
  Url                      = {http://labrosa.ee.columbia.edu/hamr2013/proceedings/doku.php/singing_separation}
}

@comment{jabref-meta: groupsversion:3;}

@comment{jabref-meta: groupstree:
0 AllEntriesGroup:;
1 ExplicitGroup:note transcription\;0\;mcnab1995signal\;molina2015sipt
h\;;
}



% This file was created with JabRef 2.10.
% Encoding: MacRoman


@article{wong2007automatic,
  title={Automatic lyrics alignment for Cantonese popular music},
  author={Wong, Chi Hang and Szeto, Wai Man and Wong, Kin Hong},
  journal={Multimedia Systems},
  volume={12},
  number={4-5},
  pages={307--323},
  year={2007},
  publisher={Springer}
}


@InProceedings{chen2012chord,
  Title                    = {Chord Recognition Using Duration-explicit Hidden Markov Models.},
  Author                   = {Chen, Ruofeng and Shen, Weibin and Srinivasamurthy, Ajay and Chordia, Parag},
  Booktitle                = {Proceedings of the 13th International Society for Music Information Retrieval Conference},
  Year                     = {2012},
  Pages                    = {445--450},

  Review                   = {duraiotn ditribution p(d)
- has same max Dur D=20 beats for all chords.
- approximated by counts of occurence ( for each chord )}
}

@InProceedings{frostel2011vowel,
  Title                    = {The Vowel Worm: Real-time Mapping and Visualisation of Sung Vowels in Music},
  Author                   = {Frostel, Harald and Arzt, Andreas and Widmer, Gerhard},
  Booktitle                = {Proceedings of the 8th Sound and Music Computing Conference},
  Year                     = {2011},
  Pages                    = {214--219}
}

@Article{fujihara2011lyricsynchronizer,
  Title                    = {LyricSynchronizer: Automatic synchronization system between musical audio signals and lyrics},
  Author                   = {Fujihara, Hiromasa and Goto, Masataka and Ogata, Jun and Okuno, Hiroshi G},
  Journal                  = {IEEE Journal of Selected Topics in Signal Processing},
  Year                     = {2011},
  Number                   = {6},
  Pages                    = {1252--1261},
  Volume                   = {5},

  Publisher                = {IEEE},
  Review                   = {1. vocal segregation: 
- find f-0 contour of most predominant melody. extract harmonic structure crresponding to melody. resynthesize melodic line
how does the newly sznthesized sound keeps the formants for the vowels. Are they based on the harmonics ? How does it sound?

2. run voice/non-voice HMM detector
- based on a novel feature

3. detect fricative sounds in the original sygnal (s, z, f, )-
- because and as non-voiced have no harmonic structure and thus are not detected by the vocal segr. module. 
fricatives are longer
Then at the candidate timestamps of fricatives on the alignment a fricative is imposed.


Section II.D - Viterbi alignment: 

only the vocals are aligned

 Adaptation is based on: 
MLLR, MAP but how: which for which phonemes?}
}

@InProceedings{janer2013separation,
  Title                    = {Separation of unvoiced fricatives in singing voice mixtures with semi-supervised NMF},
  Author                   = {Janer, Jordi and Marxer, Ricard},
  Booktitle                = {Proc. 16th International Conference on Digital Audio Effects (DAFx)},
  Year                     = {2013}
}

@InProceedings{Loscos99low-delaysinging,
  Title                    = {Low-Delay Singing Voice Alignment to Text},
  Author                   = {Alex Loscos and Pedro Cano and Jordi Bonada},
  Booktitle                = {Proceedings of the ICMC},
  Year                     = {1999}
}

@InCollection{muller2007lyrics,
  Title                    = {Lyrics-based audio retrieval and multimodal navigation in music collections},
  Author                   = {M{\"u}ller, Meinard and Kurth, Frank and Damm, David and Fremerey, Christian and Clausen, Michael},
  Booktitle                = {Research and Advanced Technology for Digital Libraries},
  Publisher                = {Springer},
  Year                     = {2007},
  Pages                    = {112--123}
}

@InProceedings{Mesaros96automaticalignment,
  title={Automatic alignment of music audio and lyrics},
  author={Mesaros, Annamaria and Virtanen, Tuomas},
  booktitle={Proceedings of the 11th Int. Conference on Digital Audio Effects (DAFx-08)},
  year={2008}
}

@Article{rabiner1989tutorial,
  Title                    = {A tutorial on hidden {Markov} models and selected applications in speech recognition},
  Author                   = {Rabiner, Lawrence},
  Journal                  = {Proceedings of the IEEE},
  Year                     = {1989},
  Number                   = {2},
  Pages                    = {257--286},
  Volume                   = {77},

  Publisher                = {IEEE}
}

@Article{salamon2012melody,
  Title                    = {Melody extraction from polyphonic music signals using pitch contour characteristics},
  Author                   = {Salamon, Justin and G{\'o}mez, Emilia},
  Journal                  = {IEEE Transactions on Audio, Speech, and Language Processing},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {1759--1770},
  Volume                   = {20},

  Publisher                = {IEEE}
}

@TechReport{Serra89asystem,
  Title                    = {A System for Sound Analysis/Transformation/Synthesis Based on a Deterministic Plus Stochastic Decomposition},
  Author                   = {Xavier Serra},
  Year                     = {1989}
}

@InProceedings{wang2004lyrically,
  Title                    = {LyricAlly: automatic synchronization of acoustic musical signals and textual lyrics},
  Author                   = {Wang, Ye and Kan, Min-Yen and Nwe, Tin Lay and Shenoy, Arun and Yin, Jun},
  Booktitle                = {Proceedings of the 12th annual ACM international conference on Multimedia},
  Year                     = {2004},
  Organization             = {ACM},
  Pages                    = {212--219},

  Review                   = {three modules: 

beat detector -> chorus Detector -> vocal detector 

Section 4.3. Vocal detector. 
Based on HMM of vocal only and non-vocal spectral distribution. Time resolution: inter-beat interval.

Section 5.1 Section PRocessor
Text processing is done (longest subsequence) to label chorus sections. 
phoneme durations learned from annotated sining voice dataset. duraiton is modeled by a distrubution of tis instances.

Section 5.2. bar-detector decodes real-time duration of bars and fits lyrics into these bars.}
}

@Book{young1993htk,
  Title                    = {The HTK hidden Markov model toolkit: Design and philosophy},
  Author                   = {Young, Steve J},
  Year                     = {1993}
}

@Article{,
  Title                    = {masataka article},

  Owner                    = {joro},
  Review                   = {It is known that the singing voice has more
complicated frequency and dynamic characteristics than speech [20]. For example,
ßuctuation of fundamental frequency (F0) 2 and loudness of singing voices are far stronger
than those of speech sounds.},
  Timestamp                = {2013.05.01},
  Url                      = {http://drops.dagstuhl.de/opus/volltexte/2012/3464/pdf/3.pdf}
}

@inproceedings{dzhambazov2014automatic,
  title={Automatic lyrics-to-audio alignment in classical {T}urkish music},
author={Dzhambazov, Georgi and Sent{\"u}rk, Sertan and Serra, Xavier},
  booktitle={The 4th International Workshop on Folk Music Analysis},
  pages={61--64},
  year={2014}
}


@article{sundberg1990science,
  title={The science of singing voice},
  author={Sundberg, Johan and Rossing, Thomas D},
  journal={the Journal of the Acoustical Society of America},
  volume={87},
  number={1},
  pages={462--463},
  year={1990},
  publisher={ASA}
}


@inproceedings{anguera2014audio,
  title={Audio-to-text alignment for speech recognition with very limited resources.},
  author={Anguera, Xavier and Luque, Jordi and Gracia, Ciro},
  booktitle={INTERSPEECH},
  pages={1405--1409},
  year={2014}
}


@inproceedings{kruspe2015keyword,
  title={Keyword spotting in singing with duration-modeled HMMs},
  author={Kruspe, Anna M},
  booktitle={Signal Processing Conference (EUSIPCO), 2015 23rd European},
  pages={1291--1295},
  year={2015},
  organization={IEEE}
}


@inproceedings{repetto2014creating,
  title={Creating a Corpus of Jingju (Beijing Opera) Music and Possibilities for Melodic Analysis.},
  author={Repetto, Rafael Caro and Serra, Xavier},
  booktitle={Proceedings of the 15th International Society for Music Information Retrieval Conference},
  pages={313--318},
  year={2014}
}


@inproceedings{chan2015vocal,
  title={Vocal activity informed singing voice separation with the iKala dataset},
  author={Chan, Tak-Shing and Yeh, Tzu-Chun and Fan, Zhe-Cheng and Chen, Hung-Wei and Su, Li and Yang, Yi-Hsuan and Jang, Roger},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={718--722},
  year={2015},
  organization={IEEE}
}

@inproceedings{lehner2015low,
  title={A low-latency, real-time-capable singing voice detection method with LSTM recurrent neural networks},
  author={Lehner, Bernhard and Widmer, Gerhard and Bock, Sebastian},
  booktitle={Signal Processing Conference (EUSIPCO), 2015 23rd European},
  pages={21--25},
  year={2015},
  organization={IEEE}
}

